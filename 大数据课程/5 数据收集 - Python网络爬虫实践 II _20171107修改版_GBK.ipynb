{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1, align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据科学引论 - Python之道 </h1>\n",
    "\n",
    "<h1, align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第5课 数据收集 - Python网络爬虫实践 II </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "接下来，我们通过一个更加复杂但是贴近实际的爬虫例子讲述。\n",
    "\n",
    "这次爬取的内容是 IT桔子 中的新公司成立内容。也就是网站http://www.itjuzi.com/company?sortby=foundtime&page=1  的前10页。\n",
    "\n",
    "爬取内容包括公司名、公司类别成立时间、省份、最新融资情况。最终以csv格式保存到文件。\n",
    "\n",
    "> csv格式是一个常见的存储表格数据的格式，爬虫完成之后的csv文件，可以用excel直接打开。\n",
    "\n",
    "## 注意\n",
    "这个爬虫比之前的样例更加复杂，因为实际的网站中，可能在解析之后要通过一些字符串操作才能得到有效信息，如网页中常出现一些空格和换行来达到良好的显示效果，但是我们爬取的时候是要将这些字符去除。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "\n",
    "    name = \"spider\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.file = open('demo2_newCompanies.csv', 'w',\n",
    "                         encoding='GBK', newline='')\n",
    "        self.csvWriter = csv.DictWriter(\n",
    "            self.file, fieldnames=['name', 'type', 'date', 'province'])\n",
    "\n",
    "        # 设置待爬取网站列表\n",
    "        self.urls = []\n",
    "        for i in range(1, 10):\n",
    "            self.urls.append(\n",
    "                'http://www.itjuzi.com/company?sortby=foundtime&page=' + str(i))\n",
    "        print(self.urls)\n",
    "\n",
    "    def start_requests(self):\n",
    "        # self.init_urls()\n",
    "        for url in self.urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # parse方法会在每个request收到response之后调用\n",
    "    def parse(self, response):\n",
    "\n",
    "        # print(response.body)\n",
    "\n",
    "        # 提取公司列表\n",
    "        companys = response.css(\".list-main-icnset.company-list-ul li\")\n",
    "\n",
    "        # 从一开始是为了跳过网页内的表格标题栏\n",
    "        for company in companys:\n",
    "            # 解析公司名\n",
    "            name = company.css(\".title span::text\").extract_first()\n",
    "            # 跳过没有公司名的公司\n",
    "            if name is None:\n",
    "                continue\n",
    "\n",
    "            # 解析公司大类\n",
    "            type = company.css(\".cell.classify::text\").extract_first()\n",
    "            # 去除网页原有的空格、换行、制表符\n",
    "            type = type.replace('\\t', '').replace('\\n', '').replace(' ', '')\n",
    "            # 解析得到时间\n",
    "            date = company.css(\".date::text\").extract_first()\n",
    "            # 去除网页原有的空格、换行、制表符\n",
    "            date = date.replace('\\t', '').replace('\\n', '').replace(' ', '')\n",
    "            # 解析省份\n",
    "            province = company.css(\".cell.place::text\").extract_first()\n",
    "            province = province.replace('\\t', '').replace(\n",
    "                '\\n', '').replace(' ', '')\n",
    "            # 构建字典\n",
    "            item = {\"name\": name, \"type\": type, \"date\": date,\n",
    "                    \"province\": province}\n",
    "\n",
    "            # 以csv格式写入文件\n",
    "            self.csvWriter.writerow(item)\n",
    "\n",
    "        # 及时将内容写入文件，否则可能会出现少许延迟\n",
    "        self.file.flush()\n",
    "        os.fsync(self.file)\n",
    "        # 输出当前解析完成的网页网址，可以当做爬取进度来看待,与程序逻辑无关\n",
    "        print(\"over: \" + response.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-07 19:37:49 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n",
      "2017-11-07 19:37:49 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2017-11-07 19:37:49 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-11-07 19:37:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-11-07 19:37:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-11-07 19:37:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-11-07 19:37:49 [scrapy.core.engine] INFO: Spider opened\n",
      "2017-11-07 19:37:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-11-07 19:37:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.itjuzi.com/company?sortby=foundtime&page=1', 'http://www.itjuzi.com/company?sortby=foundtime&page=2', 'http://www.itjuzi.com/company?sortby=foundtime&page=3', 'http://www.itjuzi.com/company?sortby=foundtime&page=4', 'http://www.itjuzi.com/company?sortby=foundtime&page=5', 'http://www.itjuzi.com/company?sortby=foundtime&page=6', 'http://www.itjuzi.com/company?sortby=foundtime&page=7', 'http://www.itjuzi.com/company?sortby=foundtime&page=8', 'http://www.itjuzi.com/company?sortby=foundtime&page=9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-07 19:37:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.itjuzi.com/company?sortby=foundtime&page=6> from <GET http://www.itjuzi.com/company?sortby=foundtime&page=6>\n",
      "2017-11-07 19:37:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.itjuzi.com/company?sortby=foundtime&page=5> from <GET http://www.itjuzi.com/company?sortby=foundtime&page=5>\n",
      "2017-11-07 19:37:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.itjuzi.com/company?sortby=foundtime&page=8> from <GET http://www.itjuzi.com/company?sortby=foundtime&page=8>\n",
      "2017-11-07 19:37:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.itjuzi.com/company?sortby=foundtime&page=1> from <GET http://www.itjuzi.com/company?sortby=foundtime&page=1>\n",
      "2017-11-07 19:37:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.itjuzi.com/company?sortby=foundtime&page=4> from <GET http://www.itjuzi.com/company?sortby=foundtime&page=4>\n",
      "2017-11-07 19:37:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.itjuzi.com/company?sortby=foundtime&page=3> from <GET http://www.itjuzi.com/company?sortby=foundtime&page=3>\n",
      "2017-11-07 19:37:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.itjuzi.com/company?sortby=foundtime&page=7> from <GET http://www.itjuzi.com/company?sortby=foundtime&page=7>\n",
      "2017-11-07 19:37:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.itjuzi.com/company?sortby=foundtime&page=2> from <GET http://www.itjuzi.com/company?sortby=foundtime&page=2>\n",
      "2017-11-07 19:37:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.itjuzi.com/company?sortby=foundtime&page=9> from <GET http://www.itjuzi.com/company?sortby=foundtime&page=9>\n",
      "2017-11-07 19:37:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.itjuzi.com/company?sortby=foundtime&page=7> (referer: None)\n",
      "2017-11-07 19:37:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.itjuzi.com/company?sortby=foundtime&page=8> (referer: None)\n",
      "2017-11-07 19:37:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.itjuzi.com/company?sortby=foundtime&page=5> (referer: None)\n",
      "2017-11-07 19:37:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.itjuzi.com/company?sortby=foundtime&page=2> (referer: None)\n",
      "2017-11-07 19:37:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.itjuzi.com/company?sortby=foundtime&page=1> (referer: None)\n",
      "2017-11-07 19:37:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.itjuzi.com/company?sortby=foundtime&page=4> (referer: None)\n",
      "2017-11-07 19:37:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.itjuzi.com/company?sortby=foundtime&page=3> (referer: None)\n",
      "2017-11-07 19:37:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.itjuzi.com/company?sortby=foundtime&page=6> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: https://www.itjuzi.com/company?sortby=foundtime&page=7\n",
      "over: https://www.itjuzi.com/company?sortby=foundtime&page=8\n",
      "over: https://www.itjuzi.com/company?sortby=foundtime&page=5\n",
      "over: https://www.itjuzi.com/company?sortby=foundtime&page=2\n",
      "over: https://www.itjuzi.com/company?sortby=foundtime&page=1\n",
      "over: https://www.itjuzi.com/company?sortby=foundtime&page=4\n",
      "over: https://www.itjuzi.com/company?sortby=foundtime&page=3\n",
      "over: https://www.itjuzi.com/company?sortby=foundtime&page=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-07 19:37:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.itjuzi.com/company?sortby=foundtime&page=9> (referer: None)\n",
      "2017-11-07 19:37:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2017-11-07 19:37:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5121,\n",
      " 'downloader/request_count': 18,\n",
      " 'downloader/request_method_count/GET': 18,\n",
      " 'downloader/response_bytes': 124904,\n",
      " 'downloader/response_count': 18,\n",
      " 'downloader/response_status_count/200': 9,\n",
      " 'downloader/response_status_count/301': 9,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 11, 7, 11, 37, 53, 727204),\n",
      " 'log_count/DEBUG': 19,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 9,\n",
      " 'scheduler/dequeued': 18,\n",
      " 'scheduler/dequeued/memory': 18,\n",
      " 'scheduler/enqueued': 18,\n",
      " 'scheduler/enqueued/memory': 18,\n",
      " 'start_time': datetime.datetime(2017, 11, 7, 11, 37, 49, 985047)}\n",
      "2017-11-07 19:37:53 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: https://www.itjuzi.com/company?sortby=foundtime&page=9\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(MySpider)\n",
    "process.start() # 这句代码就是开始了整个爬虫过程 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
