{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1, align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据科学引论 - Python之道 </h1>\n",
    "\n",
    "<h1, align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第5课 数据收集 - Python网络爬虫实践 I </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 爬虫概述\n",
    "在阅读这个样例之前，建议先了解爬虫是什么，简单理解url、爬虫技术、网页html等基本概念，这可以参考链接http://python.jobbole.com/81334/\n",
    "\n",
    "在完成本笔记本的操作之前，需要先阅读“5 爬虫环境搭建.pdf”，下载本笔记本所依赖的Python爬虫Scrapy。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义爬虫的任务\n",
    "\n",
    "## 涉及的语法\n",
    "语法涉及类（面向对象）、列表list、字典dict、循环、函数、字符串操作、文件读写\n",
    "\n",
    "## 概述\n",
    "这个爬虫的任务是爬取http://quotes.toscrape.com/page/1/ 的前两页，提取每条名言的文字内容，作者和标签，最后以JSON格式保存到文件中\n",
    "\n",
    "\n",
    "## 如何修改\n",
    "\n",
    "在自己做定制时，只需要修改`__init__`和`parse`两个方法，通俗讲__init__方法决定了爬取哪些网站，parse则指明了在每一个网页上爬取哪些内容\n",
    "- init_urls: 设置待爬取网站的列表和保存文件路径，其中变量self.urls是待爬取网站的列表，self.file是一个文件对象\n",
    "- parse：方法内是针对每个url成功访问之后进行的页面解析\n",
    "   关于如何解析具体网页，也就是选择器的使用，与网页格式十分相关，这个样例无法适用于其他网站。由于选择器的使用有很大的选择性，所以可以参考文档http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/selectors.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    \n",
    "    name = \"spider\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.file = open('demo1_quotes.json', 'w');\n",
    "        \n",
    "        #设置待爬取网站列表\n",
    "        self.urls = []\n",
    "        for i in range(1,3):\n",
    "            self.urls.append('http://quotes.toscrape.com/page/' + str(i) )\n",
    "            \n",
    "#       初始化效果 效果等同\n",
    "#         self.urls = [\n",
    "#             'http://quotes.toscrape.com/page/1/',\n",
    "#             'http://quotes.toscrape.com/page/2/',\n",
    "#         ]\n",
    "        \n",
    "        print(self.urls)\n",
    "\n",
    "        \n",
    "    def start_requests(self):\n",
    "        #self.init_urls()\n",
    "        for url in self.urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)    \n",
    "    \n",
    "\n",
    "    #parse方法会在每个request收到response之后调用\n",
    "    def parse(self, response):\n",
    "        \n",
    "        #提取名言列表\n",
    "        quotes = response.css(\"div.quote\");\n",
    "        for quote in quotes:\n",
    "            #提取每条名言中的作者名\n",
    "            author = quote.css(\"small.author::text\").extract_first();\n",
    "            #提取名言的文字内容\n",
    "            text = quote.css(\".text::text\").extract_first();\n",
    "            #提取名言标签\n",
    "            tags = quote.css(\".tags .tag::text\").extract();\n",
    "            #构建字典对象\n",
    "            item = {\"author\":author, \"text\": text, \"tags\":tags };\n",
    "            #将字典转换成json字符串\n",
    "            line = json.dumps(dict(item))\n",
    "            #将每个条目写入文件\n",
    "            self.file.write(line + \"\\n\")\n",
    "        #及时将内容写入文件，否则可能会出现少许延迟\n",
    "        self.file.flush()\n",
    "        os.fsync(self.file)\n",
    "        #输出当前解析完成的网页网址，可以当做爬取进度来看待,与程序逻辑无关\n",
    "        print(\"over: \" + response.url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行爬虫任务\n",
    "启动后，将执行Myspider。\n",
    "这部分的代码块，如果确实非常了解scrapy的运行机制，那么可以做定制，否则不建议自行修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-07 18:35:46 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n",
      "2017-11-07 18:35:46 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2017-11-07 18:35:46 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2017-11-07 18:35:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-11-07 18:35:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-11-07 18:35:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-11-07 18:35:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2017-11-07 18:35:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-11-07 18:35:46 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://quotes.toscrape.com/page/1', 'http://quotes.toscrape.com/page/2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-07 18:35:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/1/> from <GET http://quotes.toscrape.com/page/1>\n",
      "2017-11-07 18:35:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://quotes.toscrape.com/page/1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-07 18:36:46 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-11-07 18:36:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/2/> from <GET http://quotes.toscrape.com/page/2>\n",
      "2017-11-07 18:36:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
      "2017-11-07 18:36:54 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2017-11-07 18:36:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 962,\n",
      " 'downloader/request_count': 4,\n",
      " 'downloader/request_method_count/GET': 4,\n",
      " 'downloader/response_bytes': 6588,\n",
      " 'downloader/response_count': 4,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/301': 2,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 11, 7, 10, 36, 54, 490034),\n",
      " 'log_count/DEBUG': 5,\n",
      " 'log_count/INFO': 8,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 4,\n",
      " 'scheduler/dequeued/memory': 4,\n",
      " 'scheduler/enqueued': 4,\n",
      " 'scheduler/enqueued/memory': 4,\n",
      " 'start_time': datetime.datetime(2017, 11, 7, 10, 35, 46, 202478)}\n",
      "2017-11-07 18:36:54 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://quotes.toscrape.com/page/2/\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(MySpider)\n",
    "process.start() # 这句代码就是开始了整个爬虫过程 ，会输出一大堆信息，可以无视"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
