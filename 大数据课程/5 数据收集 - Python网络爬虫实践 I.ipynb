{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1, align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据科学引论 - Python之道 </h1>\n",
    "\n",
    "<h1, align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第5课 数据收集 - Python网络爬虫实践 I </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 爬虫概述\n",
    "在阅读这个样例之前，建议先了解爬虫是什么，简单理解url、爬虫技术、网页html等基本概念，这可以参考链接http://python.jobbole.com/81334/\n",
    "\n",
    "在完成本笔记本的操作之前，需要先阅读“5 爬虫环境搭建.pdf”，下载本笔记本所依赖的Python爬虫Scrapy。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义爬虫的任务\n",
    "\n",
    "## 涉及的语法\n",
    "语法涉及类（面向对象）、列表list、字典dict、循环、函数、字符串操作、文件读写\n",
    "\n",
    "## 概述\n",
    "这个爬虫的任务是爬取http://quotes.toscrape.com/page/1/ 的前两页，提取每条名言的文字内容，作者和标签，最后以JSON格式保存到文件中\n",
    "\n",
    "\n",
    "## 如何修改\n",
    "\n",
    "在自己做定制时，只需要修改`__init__`和`parse`两个方法，通俗讲__init__方法决定了爬取哪些网站，parse则指明了在每一个网页上爬取哪些内容\n",
    "- init_urls: 设置待爬取网站的列表和保存文件路径，其中变量self.urls是待爬取网站的列表，self.file是一个文件对象\n",
    "- parse：方法内是针对每个url成功访问之后进行的页面解析\n",
    "   关于如何解析具体网页，也就是选择器的使用，与网页格式十分相关，这个样例无法适用于其他网站。由于选择器的使用有很大的选择性，所以可以参考文档http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/selectors.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "\n",
    "    name = \"spider\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.file = open('demo1_quotes.json', 'w')\n",
    "\n",
    "        # 设置待爬取网站列表\n",
    "        self.urls = []\n",
    "        for i in range(1, 11):\n",
    "            self.urls.append('http://quotes.toscrape.com/page/' + str(i))\n",
    "\n",
    "#       初始化效果 效果等同\n",
    "#         self.urls = [\n",
    "#             'http://quotes.toscrape.com/page/1/',\n",
    "#             'http://quotes.toscrape.com/page/2/',\n",
    "#         ]\n",
    "\n",
    "        print(self.urls)\n",
    "\n",
    "    def start_requests(self):\n",
    "        # self.init_urls()\n",
    "        for url in self.urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # parse方法会在每个request收到response之后调用\n",
    "    def parse(self, response):\n",
    "\n",
    "        # 提取名言列表\n",
    "        quotes = response.css(\"div.quote\")\n",
    "        for quote in quotes:\n",
    "            # 提取每条名言中的作者名\n",
    "            author = quote.css(\"small.author::text\").extract_first()\n",
    "            # 提取名言的文字内容\n",
    "            text = quote.css(\".text::text\").extract_first()\n",
    "            # 提取名言标签\n",
    "            tags = quote.css(\".tags .tag::text\").extract()\n",
    "            # 构建字典对象\n",
    "            item = {\"author\": author, \"text\": text, \"tags\": tags}\n",
    "            # 将字典转换成json字符串\n",
    "            line = json.dumps(dict(item))\n",
    "            # 将每个条目写入文件\n",
    "            self.file.write(line + \"\\n\")\n",
    "        # 及时将内容写入文件，否则可能会出现少许延迟\n",
    "        self.file.flush()\n",
    "        os.fsync(self.file)\n",
    "        # 输出当前解析完成的网页网址，可以当做爬取进度来看待,与程序逻辑无关\n",
    "        print(\"over: \" + response.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from scrapy.selector import Selector\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "\n",
    "    name = \"spider\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "#         self.file = open('demo1_quotes.json', 'w')\n",
    "        # 写入csv文件\n",
    "        self.file = open('demo1_quotesAdj.csv', 'w',\n",
    "                         encoding='GBK', newline='')\n",
    "        self.csvWriter = csv.DictWriter(self.file, fieldnames=[\"author\", \"text\", \"tags\"])\n",
    "        self.csvWriter.writeheader()\n",
    "\n",
    "        # 设置待爬取网站列表\n",
    "        self.urls = []\n",
    "        for i in range(1, 11):\n",
    "            self.urls.append('http://quotes.toscrape.com/page/' + str(i))\n",
    "\n",
    "#       初始化效果 效果等同\n",
    "#         self.urls = [\n",
    "#             'http://quotes.toscrape.com/page/1/',\n",
    "#             'http://quotes.toscrape.com/page/2/',\n",
    "#         ]\n",
    "\n",
    "        print(self.urls)\n",
    "\n",
    "    def start_requests(self):\n",
    "        # self.init_urls()\n",
    "        for url in self.urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # parse方法会在每个request收到response之后调用\n",
    "    def parse(self, response):\n",
    "\n",
    "        # 提取名言列表\n",
    "        quotes = response.xpath('//div[@class=\"quote\"]').extract()\n",
    "        for quote in quotes:\n",
    "            # 重新将字符串还原为选择器\n",
    "            quote=Selector(text=quote)\n",
    "            # 提取每条名言中的作者名\n",
    "            author = quote.xpath ('//span/small[@class=\"author\"]/text()').extract_first()\n",
    "            # 提取名言的文字内容\n",
    "            text = quote.xpath ('//span[@class=\"text\"]/text()').extract_first()\n",
    "            # 提取名言标签\n",
    "            tags = quote.xpath ('//div/a[@class=\"tag\"]/text()').extract()\n",
    "            # 构建字典对象\n",
    "            item = {\"author\": author, \"text\": text, \"tags\": tags}\n",
    "            # 将字典转换成json字符串\n",
    "            line = json.dumps(dict(item))\n",
    "            # 将每个条目写入文件\n",
    "#             self.file.write(line + \"\\n\")\n",
    "            self.csvWriter.writerow(item)\n",
    "            \n",
    "        # 及时将内容写入文件，否则可能会出现少许延迟\n",
    "        self.file.flush()\n",
    "        os.fsync(self.file)\n",
    "        # 输出当前解析完成的网页网址，可以当做爬取进度来看待,与程序逻辑无关\n",
    "        print(\"over: \" + response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行爬虫任务\n",
    "启动后，将执行Myspider。\n",
    "这部分的代码块，如果确实非常了解scrapy的运行机制，那么可以做定制，否则不建议自行修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 14:32:14 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
      "2019-10-29 14:32:14 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.6.1, Platform Windows-10-10.0.18362-SP0\n",
      "2019-10-29 14:32:14 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2019-10-29 14:32:14 [scrapy.extensions.telnet] INFO: Telnet Password: 67cea940ddcb9780\n",
      "2019-10-29 14:32:14 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://quotes.toscrape.com/page/1', 'http://quotes.toscrape.com/page/2', 'http://quotes.toscrape.com/page/3', 'http://quotes.toscrape.com/page/4', 'http://quotes.toscrape.com/page/5', 'http://quotes.toscrape.com/page/6', 'http://quotes.toscrape.com/page/7', 'http://quotes.toscrape.com/page/8', 'http://quotes.toscrape.com/page/9', 'http://quotes.toscrape.com/page/10']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 14:32:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-29 14:32:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-29 14:32:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-29 14:32:15 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-29 14:32:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-10-29 14:32:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-10-29 14:32:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/2/> from <GET http://quotes.toscrape.com/page/2>\n",
      "2019-10-29 14:32:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/8/> from <GET http://quotes.toscrape.com/page/8>\n",
      "2019-10-29 14:32:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/5/> from <GET http://quotes.toscrape.com/page/5>\n",
      "2019-10-29 14:32:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/1/> from <GET http://quotes.toscrape.com/page/1>\n",
      "2019-10-29 14:32:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/4/> from <GET http://quotes.toscrape.com/page/4>\n",
      "2019-10-29 14:32:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/7/> from <GET http://quotes.toscrape.com/page/7>\n",
      "2019-10-29 14:32:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/6/> from <GET http://quotes.toscrape.com/page/6>\n",
      "2019-10-29 14:32:16 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/10/> from <GET http://quotes.toscrape.com/page/10>\n",
      "2019-10-29 14:32:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/8/> (referer: None)\n",
      "2019-10-29 14:32:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
      "2019-10-29 14:32:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/7/> (referer: None)\n",
      "2019-10-29 14:32:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/6/> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://quotes.toscrape.com/page/8/\n",
      "over: http://quotes.toscrape.com/page/2/\n",
      "over: http://quotes.toscrape.com/page/7/\n",
      "over: http://quotes.toscrape.com/page/6/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 14:32:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
      "2019-10-29 14:32:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/10/> (referer: None)\n",
      "2019-10-29 14:32:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/5/> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://quotes.toscrape.com/page/1/\n",
      "over: http://quotes.toscrape.com/page/10/\n",
      "over: http://quotes.toscrape.com/page/5/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 14:32:17 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/3/> from <GET http://quotes.toscrape.com/page/3>\n",
      "2019-10-29 14:32:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/4/> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://quotes.toscrape.com/page/4/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 14:32:20 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://quotes.toscrape.com/page/9/> from <GET http://quotes.toscrape.com/page/9>\n",
      "2019-10-29 14:32:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/9/> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://quotes.toscrape.com/page/9/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 14:32:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/3/> (referer: None)\n",
      "2019-10-29 14:32:24 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-10-29 14:32:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 4812,\n",
      " 'downloader/request_count': 20,\n",
      " 'downloader/request_method_count/GET': 20,\n",
      " 'downloader/response_bytes': 29347,\n",
      " 'downloader/response_count': 20,\n",
      " 'downloader/response_status_count/200': 10,\n",
      " 'downloader/response_status_count/301': 10,\n",
      " 'elapsed_time_seconds': 9.315847,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 10, 29, 6, 32, 24, 552600),\n",
      " 'log_count/DEBUG': 20,\n",
      " 'log_count/INFO': 10,\n",
      " 'response_received_count': 10,\n",
      " 'scheduler/dequeued': 20,\n",
      " 'scheduler/dequeued/memory': 20,\n",
      " 'scheduler/enqueued': 20,\n",
      " 'scheduler/enqueued/memory': 20,\n",
      " 'start_time': datetime.datetime(2019, 10, 29, 6, 32, 15, 236753)}\n",
      "2019-10-29 14:32:24 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over: http://quotes.toscrape.com/page/3/\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(MySpider)\n",
    "process.start()  # 这句代码就是开始了整个爬虫过程 ，会输出一大堆信息，可以无视"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Alfred Tennyson</td>\n",
       "      <td>“If I had a flower for every time I thought of...</td>\n",
       "      <td>['friendship', 'love']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Charles Bukowski</td>\n",
       "      <td>“Some people never go crazy. What truly horrib...</td>\n",
       "      <td>['humor']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Terry Pratchett</td>\n",
       "      <td>“The trouble with having an open mind, of cour...</td>\n",
       "      <td>['humor', 'open-mind', 'thinking']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>“Think left and think right and think low and ...</td>\n",
       "      <td>['humor', 'philosophy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>J.D. Salinger</td>\n",
       "      <td>“What really knocks me out is a book that, whe...</td>\n",
       "      <td>['authors', 'books', 'literature', 'reading', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>“Today you are You, that is truer than true. T...</td>\n",
       "      <td>['comedy', 'life', 'yourself']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>“If you want your children to be intelligent, ...</td>\n",
       "      <td>['children', 'fairy-tales']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>“It is impossible to live without failing at s...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>“Logic will get you from A to Z; imagination w...</td>\n",
       "      <td>['imagination']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Bob Marley</td>\n",
       "      <td>“One good thing about music, when it hits you,...</td>\n",
       "      <td>['music']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               text  \\\n",
       "author                                                                \n",
       "Alfred Tennyson   “If I had a flower for every time I thought of...   \n",
       "Charles Bukowski  “Some people never go crazy. What truly horrib...   \n",
       "Terry Pratchett   “The trouble with having an open mind, of cour...   \n",
       "Dr. Seuss         “Think left and think right and think low and ...   \n",
       "J.D. Salinger     “What really knocks me out is a book that, whe...   \n",
       "...                                                             ...   \n",
       "Dr. Seuss         “Today you are You, that is truer than true. T...   \n",
       "Albert Einstein   “If you want your children to be intelligent, ...   \n",
       "J.K. Rowling      “It is impossible to live without failing at s...   \n",
       "Albert Einstein   “Logic will get you from A to Z; imagination w...   \n",
       "Bob Marley        “One good thing about music, when it hits you,...   \n",
       "\n",
       "                                                               tags  \n",
       "author                                                               \n",
       "Alfred Tennyson                              ['friendship', 'love']  \n",
       "Charles Bukowski                                          ['humor']  \n",
       "Terry Pratchett                  ['humor', 'open-mind', 'thinking']  \n",
       "Dr. Seuss                                   ['humor', 'philosophy']  \n",
       "J.D. Salinger     ['authors', 'books', 'literature', 'reading', ...  \n",
       "...                                                             ...  \n",
       "Dr. Seuss                            ['comedy', 'life', 'yourself']  \n",
       "Albert Einstein                         ['children', 'fairy-tales']  \n",
       "J.K. Rowling                                                     []  \n",
       "Albert Einstein                                     ['imagination']  \n",
       "Bob Marley                                                ['music']  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('demo1_quotesAdj.csv',index_col=0,encoding='GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
