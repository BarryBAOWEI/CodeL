{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np;\n",
    "import os\n",
    "import jieba\n",
    "import gensim.models.word2vec as w2v\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(uchar):\n",
    "    \"\"\"判断一个unicode是否是汉字\"\"\"\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_number(uchar):\n",
    "    \"\"\"判断一个unicode是否是数字\"\"\"\n",
    "    if uchar >= u'\\u0030' and uchar <= u'\\u0039':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_alphabet(uchar):\n",
    "    \"\"\"判断一个unicode是否是英文字母\"\"\"\n",
    "    if (uchar >= u'\\u0041' and uchar <= u'\\u005a') or (uchar >= u'\\u0061' and uchar <= u'\\u007a'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_legal(uchar):\n",
    "    \"\"\"判断是否非汉字，数字和英文字符\"\"\"\n",
    "    if not (is_chinese(uchar) or is_number(uchar) or is_alphabet(uchar)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def extract_chinese(line): # 对TXT中的每一行，提取汉字数字与英文与数字\n",
    "    res = \"\"\n",
    "    for word in line:\n",
    "        if is_legal(word):\n",
    "            res += word\n",
    "    return res\n",
    "\n",
    "def words2line(words): # 对每个words，拼成一个字符串\n",
    "    line = \"\"\n",
    "    for word in words:\n",
    "        line += \" \" + word\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'体育': 0, '娱乐': 1, '家居': 2, '彩票': 3, '房产': 4, '教育': 5, '时尚': 6, '时政': 7, '星座': 8, '游戏': 9, '社会': 10, '科技': 11, '股票': 12, '财经': 13}\n",
      "体育 0\n",
      "娱乐 1\n",
      "家居 2\n",
      "彩票 3\n",
      "房产 4\n",
      "教育 5\n",
      "时尚 6\n",
      "时政 7\n",
      "星座 8\n",
      "游戏 9\n",
      "社会 10\n",
      "科技 11\n",
      "股票 12\n",
      "财经 13\n"
     ]
    }
   ],
   "source": [
    "#数据预处理函数，在dir文件夹下每个子文件是一类内容\n",
    "def datahelper(dir):\n",
    "#返回为文本，文本对应标签，标签及索引，索引及标签\n",
    "    labels_index={}\n",
    "    index_lables={}\n",
    "    fs = os.listdir(dir)\n",
    "#     MAX_SEQUENCE_LENGTH = 200\n",
    "#     MAX_NB_WORDS = 50000\n",
    "#     EMBEDDING_DIM = 20\n",
    "#     VALIDATION_SPLIT = 0.2\n",
    "    i = 0;\n",
    "    for f in fs:\n",
    "        labels_index[f] = i;\n",
    "        index_lables[i] = f\n",
    "        i = i + 1;\n",
    "    print(labels_index)\n",
    "    texts = []   # 每句话（jieba拆词后）的列表\n",
    "    labels = []  # list of label ids\n",
    "    for la in labels_index.keys():\n",
    "        print(la + \" \" + str(labels_index[la]))\n",
    "        la_dir = dir + \"/\" + la;  # 读入某一类别，例如“体育”的文件夹\n",
    "        fs = os.listdir(la_dir) # 获取全部该类文件夹下的文件名\n",
    "        for f in fs:\n",
    "            file = open(la_dir + \"/\" + f, encoding='utf-8') # 打开该txt文件\n",
    "            lines = file.readlines();\n",
    "            text = ''\n",
    "            num_recs=0\n",
    "            for line in lines:\n",
    "                if len(line) > 5: # 只提取大于5个字的语句\n",
    "                    line = extract_chinese(line) # 提取中文有效信息，汉字 字母 数字\n",
    "                    words = jieba.lcut(line, cut_all=False, HMM=True) # 对每句话拆分成词语的list\n",
    "                    text = words\n",
    "                    texts.append(text)\n",
    "                    labels.append(labels_index[la])\n",
    "                    num_recs = num_recs + 1\n",
    "    return texts,labels,labels_index,index_lables\n",
    "\n",
    "train_dir = 'D:/THUCNewsTiny'\n",
    "# train_dir = 'D:/THUCNews'\n",
    "# train_dir = 'D:/THUCNewsSmall'\n",
    "\n",
    "texts,labels,labels_index,index_lables=datahelper(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textCNN模型\n",
    "class textCNN(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(textCNN, self).__init__()\n",
    "        vocb_size = args['vocb_size']\n",
    "        dim = args['dim']\n",
    "        n_class = args['n_class']\n",
    "        max_len = args['max_len']\n",
    "        embedding_matrix=args['embedding_matrix']\n",
    "        #需要将事先训练好的词向量载入\n",
    "        self.embeding = nn.Embedding(vocb_size, dim,_weight=embedding_matrix)\n",
    "        self.conv1 = nn.Sequential(\n",
    "                     nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5,\n",
    "                               stride=1, padding=2),\n",
    "\n",
    "                     nn.ReLU(),\n",
    "                     nn.MaxPool2d(kernel_size=2) # (16,64,64)\n",
    "                     )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                     nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "                     nn.ReLU(),\n",
    "                     nn.MaxPool2d(2)\n",
    "                     )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(  # (16,64,64)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.out = nn.Linear(1024, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeding(x)\n",
    "        x=x.view(x.size(0),1,max_len,word_dim)\n",
    "        #print(x.size())\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.size(0), -1) # 将（batch，outchanel,w,h）展平为（batch，outchanel*w*h）\n",
    "        #print(x.size())\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词表\n",
    "word_all=[]\n",
    "word_all.append('')\n",
    "for text in texts: \n",
    "    for word in text:  # 拿出每句话中的每个词，作为词表组成元素\n",
    "        word_all.append(word)\n",
    "word_vocb=set(word_all)\n",
    "vocb_size=len(word_vocb)\n",
    "\n",
    "#设置词表大小\n",
    "nb_words=40000\n",
    "max_len=64; # 一句话最多64个词！\n",
    "word_dim=40; # 64x40就是输入矩阵的大小，对应一句话\n",
    "n_class=len(index_lables)\n",
    "\n",
    "args={}\n",
    "if nb_words<vocb_size:\n",
    "    nb_words=vocb_size;\n",
    "\n",
    "    #textCNN调用的参数\n",
    "args['vocb_size']=nb_words\n",
    "args['max_len']=max_len\n",
    "args['n_class']=n_class\n",
    "args['dim']=word_dim\n",
    "\n",
    "texts_with_id=np.zeros([len(texts),max_len])\n",
    "\n",
    "#词表与索引的map\n",
    "word_to_idx={word:i for i,word in enumerate(word_vocb)}\n",
    "idx_to_word={word_to_idx[word]:word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#每个单词的对应的词向量  load word 2 vetc，加载词向量，可以事先预训练\n",
    "embeddings_index = w2v.Word2Vec.load('D:/THUNewsAllWord/THUCNewsw2v.pkl')\n",
    "#预先处理好的词向量\n",
    "embedding_matrix = np.zeros((nb_words, word_dim)) # 每个词的词向量生成表\n",
    "for word, i in word_to_idx.items():\n",
    "#     if i >= nb_words:\n",
    "#         continue\n",
    "    if word in embeddings_index:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "#         if int(sum(embedding_vector)) != 0:\n",
    "#             words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] += embedding_vector\n",
    "args['embedding_matrix']=torch.Tensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成训练数据，需要将训练数据的Word转换为word的索引\n",
    "for i in range(0,len(texts)):\n",
    "    if len(texts[i])<max_len:\n",
    "        for j in range(0,len(texts[i])):\n",
    "            texts_with_id[i][j]=word_to_idx[texts[i][j]]\n",
    "        for j in range(len(texts[i]),max_len):\n",
    "            texts_with_id[i][j] = word_to_idx['']\n",
    "    else:\n",
    "        for j in range(0,max_len):\n",
    "            texts_with_id[i][j]=word_to_idx[texts[i][j]]\n",
    "            \n",
    "# 生成的texts_with_id 每行是原文本的每句话，列宽限定在max_len，每行变为原每个词对应的词编号0~XXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56356\n"
     ]
    }
   ],
   "source": [
    "#构建textCNN模型 - LSC\n",
    "# cnn=textCNN(args)\n",
    "\n",
    "LR = 0.001\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "\n",
    "#损失函数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#训练批次大小\n",
    "# epoch_size=1000;\n",
    "texts_len=len(texts_with_id)\n",
    "print(texts_len)\n",
    "\n",
    "#划分训练数据和测试数据\n",
    "x_train, x_test, y_train, y_test = train_test_split(texts_with_id, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = torch.LongTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "x_test = torch.LongTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1000,shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=300,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Step: 1 Finished!\n",
      "Step: 2 Finished!\n",
      "Step: 3 Finished!\n",
      "Step: 4 Finished!\n",
      "Step: 5 Finished!\n",
      "Step: 6 Finished!\n",
      "Step: 7 Finished!\n",
      "Step: 8 Finished!\n",
      "Step: 9 Finished!\n",
      "Step: 10 Finished!\n",
      "Step: 11 Finished!\n",
      "Step: 12 Finished!\n",
      "Step: 13 Finished!\n",
      "Step: 14 Finished!\n",
      "Step: 15 Finished!\n",
      "Step: 16 Finished!\n",
      "Step: 17 Finished!\n",
      "Step: 18 Finished!\n",
      "Step: 19 Finished!\n",
      "Step: 20 Finished!\n",
      "Step: 21 Finished!\n",
      "Step: 22 Finished!\n",
      "Step: 23 Finished!\n",
      "Step: 24 Finished!\n",
      "Step: 25 Finished!\n",
      "Step: 26 Finished!\n",
      "Step: 27 Finished!\n",
      "Step: 28 Finished!\n",
      "Step: 29 Finished!\n",
      "Step: 30 Finished!\n",
      "Step: 31 Finished!\n",
      "Step: 32 Finished!\n",
      "Step: 33 Finished!\n",
      "Step: 34 Finished!\n",
      "Step: 35 Finished!\n",
      "Step: 36 Finished!\n",
      "Step: 37 Finished!\n",
      "Step: 38 Finished!\n",
      "Step: 39 Finished!\n",
      "Step: 40 Finished!\n",
      "Step: 41 Finished!\n",
      "Step: 42 Finished!\n",
      "Step: 43 Finished!\n",
      "Step: 44 Finished!\n",
      "Step: 45 Finished!\n",
      "Step: 46 Finished!\n",
      "Train Acc: 0.990329\n",
      "Test Acc: 0.825053\n",
      "epoch 2\n",
      "Step: 1 Finished!\n",
      "Step: 2 Finished!\n",
      "Step: 3 Finished!\n",
      "Step: 4 Finished!\n",
      "Step: 5 Finished!\n",
      "Step: 6 Finished!\n",
      "Step: 7 Finished!\n",
      "Step: 8 Finished!\n",
      "Step: 9 Finished!\n",
      "Step: 10 Finished!\n",
      "Step: 11 Finished!\n",
      "Step: 12 Finished!\n",
      "Step: 13 Finished!\n",
      "Step: 14 Finished!\n",
      "Step: 15 Finished!\n",
      "Step: 16 Finished!\n",
      "Step: 17 Finished!\n",
      "Step: 18 Finished!\n",
      "Step: 19 Finished!\n",
      "Step: 20 Finished!\n",
      "Step: 21 Finished!\n",
      "Step: 22 Finished!\n",
      "Step: 23 Finished!\n",
      "Step: 24 Finished!\n",
      "Step: 25 Finished!\n",
      "Step: 26 Finished!\n",
      "Step: 27 Finished!\n",
      "Step: 28 Finished!\n",
      "Step: 29 Finished!\n",
      "Step: 30 Finished!\n",
      "Step: 31 Finished!\n",
      "Step: 32 Finished!\n",
      "Step: 33 Finished!\n",
      "Step: 34 Finished!\n",
      "Step: 35 Finished!\n",
      "Step: 36 Finished!\n",
      "Step: 37 Finished!\n",
      "Step: 38 Finished!\n",
      "Step: 39 Finished!\n",
      "Step: 40 Finished!\n",
      "Step: 41 Finished!\n",
      "Step: 42 Finished!\n",
      "Step: 43 Finished!\n",
      "Step: 44 Finished!\n",
      "Step: 45 Finished!\n",
      "Step: 46 Finished!\n",
      "Train Acc: 0.989952\n",
      "Test Acc: 0.827271\n",
      "epoch 3\n",
      "Step: 1 Finished!\n",
      "Step: 2 Finished!\n",
      "Step: 3 Finished!\n",
      "Step: 4 Finished!\n",
      "Step: 5 Finished!\n",
      "Step: 6 Finished!\n",
      "Step: 7 Finished!\n",
      "Step: 8 Finished!\n",
      "Step: 9 Finished!\n",
      "Step: 10 Finished!\n",
      "Step: 11 Finished!\n",
      "Step: 12 Finished!\n",
      "Step: 13 Finished!\n",
      "Step: 14 Finished!\n",
      "Step: 15 Finished!\n",
      "Step: 16 Finished!\n",
      "Step: 17 Finished!\n",
      "Step: 18 Finished!\n",
      "Step: 19 Finished!\n",
      "Step: 20 Finished!\n",
      "Step: 21 Finished!\n",
      "Step: 22 Finished!\n",
      "Step: 23 Finished!\n",
      "Step: 24 Finished!\n",
      "Step: 25 Finished!\n",
      "Step: 26 Finished!\n",
      "Step: 27 Finished!\n",
      "Step: 28 Finished!\n",
      "Step: 29 Finished!\n",
      "Step: 30 Finished!\n",
      "Step: 31 Finished!\n",
      "Step: 32 Finished!\n",
      "Step: 33 Finished!\n",
      "Step: 34 Finished!\n",
      "Step: 35 Finished!\n",
      "Step: 36 Finished!\n",
      "Step: 37 Finished!\n",
      "Step: 38 Finished!\n",
      "Step: 39 Finished!\n",
      "Step: 40 Finished!\n",
      "Step: 41 Finished!\n",
      "Step: 42 Finished!\n",
      "Step: 43 Finished!\n",
      "Step: 44 Finished!\n",
      "Step: 45 Finished!\n",
      "Step: 46 Finished!\n",
      "Train Acc: 0.993146\n",
      "Test Acc: 0.823279\n",
      "epoch 4\n",
      "Step: 1 Finished!\n",
      "Step: 2 Finished!\n",
      "Step: 3 Finished!\n",
      "Step: 4 Finished!\n",
      "Step: 5 Finished!\n",
      "Step: 6 Finished!\n",
      "Step: 7 Finished!\n",
      "Step: 8 Finished!\n",
      "Step: 9 Finished!\n",
      "Step: 10 Finished!\n",
      "Step: 11 Finished!\n",
      "Step: 12 Finished!\n",
      "Step: 13 Finished!\n",
      "Step: 14 Finished!\n",
      "Step: 15 Finished!\n",
      "Step: 16 Finished!\n",
      "Step: 17 Finished!\n",
      "Step: 18 Finished!\n",
      "Step: 19 Finished!\n",
      "Step: 20 Finished!\n",
      "Step: 21 Finished!\n",
      "Step: 22 Finished!\n",
      "Step: 23 Finished!\n",
      "Step: 24 Finished!\n",
      "Step: 25 Finished!\n",
      "Step: 26 Finished!\n",
      "Step: 27 Finished!\n",
      "Step: 28 Finished!\n",
      "Step: 29 Finished!\n",
      "Step: 30 Finished!\n",
      "Step: 31 Finished!\n",
      "Step: 32 Finished!\n",
      "Step: 33 Finished!\n",
      "Step: 34 Finished!\n",
      "Step: 35 Finished!\n",
      "Step: 36 Finished!\n",
      "Step: 37 Finished!\n",
      "Step: 38 Finished!\n",
      "Step: 39 Finished!\n",
      "Step: 40 Finished!\n",
      "Step: 41 Finished!\n",
      "Step: 42 Finished!\n",
      "Step: 43 Finished!\n",
      "Step: 44 Finished!\n",
      "Step: 45 Finished!\n",
      "Step: 46 Finished!\n",
      "Train Acc: 0.993545\n",
      "Test Acc: 0.822835\n",
      "epoch 5\n",
      "Step: 1 Finished!\n",
      "Step: 2 Finished!\n",
      "Step: 3 Finished!\n",
      "Step: 4 Finished!\n",
      "Step: 5 Finished!\n",
      "Step: 6 Finished!\n",
      "Step: 7 Finished!\n",
      "Step: 8 Finished!\n",
      "Step: 9 Finished!\n",
      "Step: 10 Finished!\n",
      "Step: 11 Finished!\n",
      "Step: 12 Finished!\n",
      "Step: 13 Finished!\n",
      "Step: 14 Finished!\n",
      "Step: 15 Finished!\n",
      "Step: 16 Finished!\n",
      "Step: 17 Finished!\n",
      "Step: 18 Finished!\n",
      "Step: 19 Finished!\n",
      "Step: 20 Finished!\n",
      "Step: 21 Finished!\n",
      "Step: 22 Finished!\n",
      "Step: 23 Finished!\n",
      "Step: 24 Finished!\n",
      "Step: 25 Finished!\n",
      "Step: 26 Finished!\n",
      "Step: 27 Finished!\n",
      "Step: 28 Finished!\n",
      "Step: 29 Finished!\n",
      "Step: 30 Finished!\n",
      "Step: 31 Finished!\n",
      "Step: 32 Finished!\n",
      "Step: 33 Finished!\n",
      "Step: 34 Finished!\n",
      "Step: 35 Finished!\n",
      "Step: 36 Finished!\n",
      "Step: 37 Finished!\n",
      "Step: 38 Finished!\n",
      "Step: 39 Finished!\n",
      "Step: 40 Finished!\n",
      "Step: 41 Finished!\n",
      "Step: 42 Finished!\n",
      "Step: 43 Finished!\n",
      "Step: 44 Finished!\n",
      "Step: 45 Finished!\n",
      "Step: 46 Finished!\n",
      "Train Acc: 0.992392\n",
      "Test Acc: 0.822924\n"
     ]
    }
   ],
   "source": [
    "# 训练是一句话一句话训练的！！ - LSC\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "if use_gpu:\n",
    "    cnn = cnn.cuda()\n",
    "\n",
    "for epoch in range(5):\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    # training-----------------------------\n",
    "    cnn.train()\n",
    "    train_acc = 0.\n",
    "#     L_train_pred = []\n",
    "#     L_train_real = []\n",
    "    for step, (batch_x, batch_y) in enumerate(train_data_loader):\n",
    "        batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "        \n",
    "        if use_gpu:\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "            \n",
    "        out = cnn(batch_x)\n",
    "        loss = loss_function(out, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = torch.max(out, 1)[1]\n",
    "        num_correct = (pred == batch_y).sum()\n",
    "        train_acc += num_correct.data\n",
    "        \n",
    "#         L_train_pred += pred.cpu().numpy().tolist()\n",
    "#         L_train_real += batch_y.cpu().numpy().tolist()\n",
    "        \n",
    "        print('Step:',step+1,'Finished!')\n",
    "    print('Train Acc: {:.6f}'.format(train_acc.cpu().numpy() / (len(train_dataset))))\n",
    "#     print(classification_report(L_train_real,L_train_pred))\n",
    "\n",
    "    # evaluation--------------------------------\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_acc = 0.\n",
    "#         L_val_pred = []\n",
    "#         L_val_real = []\n",
    "        for batch_x, batch_y in test_data_loader:\n",
    "            batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "\n",
    "            if use_gpu:\n",
    "                batch_x = batch_x.cuda()\n",
    "                batch_y = batch_y.cuda()\n",
    "\n",
    "            out = cnn(batch_x)\n",
    "            loss = loss_function(out, batch_y)\n",
    "            \n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = (pred == batch_y).sum()\n",
    "            eval_acc += num_correct\n",
    "            \n",
    "#             L_val_pred += pred.cpu().numpy().tolist()\n",
    "#             L_val_real += batch_y.cpu().numpy().tolist()\n",
    "            \n",
    "        print('Test Acc: {:.6f}'.format(eval_acc.cpu().numpy() / (len(test_dataset))))\n",
    "#         print(classification_report(L_val_real,L_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56356\n"
     ]
    }
   ],
   "source": [
    "#构建textCNN模型 - COBY\n",
    "cnn=textCNN(args)\n",
    "\n",
    "LR = 0.001\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "#损失函数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#训练批次大小\n",
    "epoch_size=1000;\n",
    "texts_len=len(texts_with_id)\n",
    "print(texts_len)\n",
    "#划分训练数据和测试数据\n",
    "x_train, x_test, y_train, y_test = train_test_split(texts_with_id, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "test_x=torch.LongTensor(x_test)\n",
    "test_y=torch.LongTensor(y_test)\n",
    "train_x=x_train\n",
    "train_y=y_train\n",
    "\n",
    "test_epoch_size=300;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc 0.751\n",
      "train acc 0.738\n",
      "train acc 0.745\n",
      "train acc 0.755\n",
      "train acc 0.763\n",
      "train acc 0.731\n",
      "train acc 0.759\n",
      "train acc 0.759\n",
      "train acc 0.75\n",
      "train acc 0.737\n",
      "train acc 0.758\n",
      "train acc 0.754\n",
      "train acc 0.781\n",
      "train acc 0.753\n",
      "train acc 0.782\n",
      "train acc 0.764\n",
      "train acc 0.765\n",
      "train acc 0.757\n",
      "train acc 0.764\n",
      "train acc 0.75\n",
      "train acc 0.802\n",
      "train acc 0.753\n",
      "train acc 0.777\n",
      "train acc 0.768\n",
      "train acc 0.784\n",
      "train acc 0.8\n",
      "train acc 0.77\n",
      "train acc 0.759\n",
      "train acc 0.794\n",
      "train acc 0.772\n",
      "train acc 0.769\n",
      "train acc 0.813\n",
      "train acc 0.784\n",
      "train acc 0.791\n",
      "train acc 0.793\n",
      "train acc 0.806\n",
      "train acc 0.809\n",
      "train acc 0.801\n",
      "train acc 0.802\n",
      "train acc 0.787\n",
      "train acc 0.796\n",
      "train acc 0.797\n",
      "train acc 0.803\n",
      "train acc 0.832\n",
      "train acc 0.807\n",
      "epoch 1 train acc 0.7737778369266258\n",
      "test acc 0.7566666666666667\n",
      "test acc 0.7333333333333333\n",
      "test acc 0.73\n",
      "test acc 0.7133333333333334\n",
      "test acc 0.7533333333333333\n",
      "test acc 0.68\n",
      "test acc 0.7033333333333334\n",
      "test acc 0.7466666666666667\n",
      "test acc 0.6733333333333333\n",
      "test acc 0.7033333333333334\n",
      "test acc 0.7466666666666667\n",
      "test acc 0.7333333333333333\n",
      "test acc 0.6433333333333333\n",
      "test acc 0.7633333333333333\n",
      "test acc 0.7266666666666667\n",
      "test acc 0.7533333333333333\n",
      "test acc 0.7566666666666667\n",
      "test acc 0.7366666666666667\n",
      "test acc 0.71\n",
      "test acc 0.71\n",
      "test acc 0.7266666666666667\n",
      "test acc 0.7766666666666666\n",
      "test acc 0.73\n",
      "test acc 0.74\n",
      "test acc 0.7233333333333334\n",
      "test acc 0.7433333333333333\n",
      "test acc 0.7533333333333333\n",
      "test acc 0.71\n",
      "test acc 0.74\n",
      "test acc 0.7633333333333333\n",
      "test acc 0.7166666666666667\n",
      "test acc 0.7333333333333333\n",
      "test acc 0.7466666666666667\n",
      "test acc 0.6966666666666667\n",
      "test acc 0.74\n",
      "test acc 0.7466666666666667\n",
      "test acc 0.75\n",
      "epoch 1 test acc 0.718860894251242\n",
      "train acc 0.841\n",
      "train acc 0.827\n",
      "train acc 0.814\n",
      "train acc 0.845\n",
      "train acc 0.833\n",
      "train acc 0.804\n",
      "train acc 0.825\n",
      "train acc 0.834\n",
      "train acc 0.823\n",
      "train acc 0.823\n",
      "train acc 0.806\n",
      "train acc 0.819\n",
      "train acc 0.854\n",
      "train acc 0.829\n",
      "train acc 0.846\n",
      "train acc 0.804\n",
      "train acc 0.83\n",
      "train acc 0.799\n",
      "train acc 0.839\n",
      "train acc 0.81\n",
      "train acc 0.842\n",
      "train acc 0.821\n",
      "train acc 0.819\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-bba305319176>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mb_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-0615593b13df>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m#print(x.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    146\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m    147\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m                             self.return_indices)\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;31m# type: (Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], BroadcastingList2[int], BroadcastingList2[int], bool, bool) -> Tensor  # noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     return max_pool2d_with_indices(\n\u001b[1;32m--> 425\u001b[1;33m         input, kernel_size, stride, padding, dilation, ceil_mode)[0]\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m max_pool2d = torch._jit_internal.boolean_dispatch(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmax_pool2d_with_indices\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0m_stride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d_with_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练是一句话一句话训练的！！ - COBY\n",
    "EPOCH=2;\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    \n",
    "    train_acc_all = 0;\n",
    "    for i in range(0,(int)(len(train_x)/epoch_size)):\n",
    "\n",
    "        b_x = Variable(torch.LongTensor(train_x[i*epoch_size:i*epoch_size+epoch_size])) # 选小训练批次的样本\n",
    "        b_y = Variable(torch.LongTensor((train_y[i*epoch_size:i*epoch_size+epoch_size])))\n",
    "        \n",
    "        output = cnn(b_x)\n",
    "        loss = loss_function(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred_y = torch.max(output, 1)[1].data.squeeze()\n",
    "        acc = (b_y == pred_y)\n",
    "        acc = acc.numpy().sum()\n",
    "        print(\"train acc \" + str(acc / b_y.size(0)))\n",
    "        train_acc_all += acc\n",
    "    \n",
    "    accuracy = train_acc_all / (len(train_x))\n",
    "    print(\"epoch \" + str(epoch+1) +\" \" + \"train acc \" + str(accuracy))\n",
    "\n",
    "    test_acc_all = 0;\n",
    "    for j in range(0, (int)(len(test_x) / test_epoch_size)):\n",
    "        \n",
    "        b_x = Variable(torch.LongTensor(test_x[j * test_epoch_size:j * test_epoch_size + test_epoch_size]))\n",
    "        b_y = Variable(torch.LongTensor((test_y[j * test_epoch_size:j * test_epoch_size + test_epoch_size])))\n",
    "        \n",
    "        test_output = cnn(b_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "        # print(pred_y)\n",
    "        # print(test_y)\n",
    "        acc = (pred_y == b_y)\n",
    "        acc = acc.numpy().sum()\n",
    "        print(\"test acc \" + str(acc / b_y.size(0)))\n",
    "        test_acc_all += acc\n",
    "\n",
    "    accuracy = test_acc_all / (test_y.size(0))\n",
    "    print(\"epoch \" + str(epoch+1) +\" \" + \"test acc \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练 词语变为词向量 - 分块保存成训练样本\n",
    "cut_bin_len = int(len(texts)/100)\n",
    "txt_root = 'D:/THUNewsAllWord/'\n",
    "n = 0\n",
    "while n<100:\n",
    "    f = open(txt_root+'THUNewsAllWord'+str(n)+'.txt','w+',encoding='utf-8')\n",
    "    for sentences_txt in texts[n*cut_bin_len:(n+1)*cut_bin_len]: # 迭代方式要改变\n",
    "        for word_txt in  sentences_txt:\n",
    "            f.write(word_txt + ' ')\n",
    "        f.write('\\r')\n",
    "    f.close()\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    }
   ],
   "source": [
    "# 预训练 词语变为词向量 - 增量训练\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.models.keyedvectors import KeyedVectors as KV\n",
    "\n",
    "big_batch_num = int(len(texts)/cut_bin_len)\n",
    "\n",
    "sentences = w2v.LineSentence(txt_root+'THUNewsAllWord'+'0'+'.txt')\n",
    "model = w2v.Word2Vec(sentences, sg=1, size=40,  window=5,  min_count=5,  negative=3, sample=0.001, hs=1, workers=4)\n",
    "model.save('D:/THUNewsAllWord/THUCNewsw2v.pkl') \n",
    "\n",
    "for big_batch in range(1,big_batch_num):\n",
    "    sentences = w2v.LineSentence(txt_root+'THUNewsAllWord'+str(big_batch)+'.txt')\n",
    "    model = KV.load('D:/THUNewsAllWord/THUCNewsw2v.pkl')\n",
    "    model.build_vocab(sentences, update=True)\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.save('D:/THUNewsAllWord/THUCNewsw2v.pkl') \n",
    "\n",
    "# model = KV.load('D:/THUCNewsSmallw2v.pkl')\n",
    "# with open('D:/THUNewsAllWord.txt',encoding='utf-8') as ffff:\n",
    "#     all_word_lines = ffff.readlines()\n",
    "#     for all_word_line in all_word_lines:\n",
    "        \n",
    "# model.save('D:/THUCNewsSmallw2v.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "模式 描述\n",
    "r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。\n",
    "rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。\n",
    "r+ 打开一个文件用于读写。文件指针将会放在文件的开头。\n",
    "rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。\n",
    "w 打开一个文件只用于写入。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。\n",
    "wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。\n",
    "w+ 打开一个文件用于读写。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。\n",
    "wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。\n",
    "a\t打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。\n",
    "ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。\n",
    "a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。\n",
    "ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
